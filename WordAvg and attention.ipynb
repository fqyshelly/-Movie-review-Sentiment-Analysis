{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/senti.train.tsv\"\n",
    "dev_path = \"data/senti.dev.tsv\"\n",
    "test_path = \"data/senti.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.corpus import wordnet\n",
    "def getCleanData(x):\n",
    "\n",
    "    # Reduce each word into common base\n",
    "    lem = WordNetLemmatizer()\n",
    "    x = [lem.lemmatize(word) for word in x] \n",
    "    x = [lem.lemmatize(word,'v') for word in x]\n",
    "    x = [lem.lemmatize(word,'r') for word in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        sents = []\n",
    "        labels = []\n",
    "        for line in f.readlines():\n",
    "            sent = line.split('\\t')[0].lower()\n",
    "            sent = getCleanData(sent.split(' '))\n",
    "            label = line.split('\\t')[1].strip('\\n')\n",
    "            sents.append(sent)\n",
    "            labels.append(label)\n",
    "    return sents, labels\n",
    "\n",
    "def build_vocab(sents):\n",
    "    dic = {}\n",
    "    word_counter = Counter()\n",
    "    dic['PAD'] = 0\n",
    "    dic['UNK'] = 1\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_counter[word] += 1\n",
    "    itos = [w for w, c in word_counter.items()]\n",
    "    for w in itos:\n",
    "        dic[w] = len(dic)\n",
    "    return dic\n",
    "\n",
    "def vectorize(sents):\n",
    "    vecs = [[wtoi.get(word, wtoi.get(\"UNK\")) for word in sent] for sent in sents]\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_label = read_corpus(train_path)\n",
    "dev_data,dev_label = read_corpus(dev_path)\n",
    "test_data,test_label = read_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtoi= build_vocab(train_data)\n",
    "itow = dict((v,k) for k, v in wtoi.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = vectorize(train_data)\n",
    "dev_set = vectorize(dev_data)\n",
    "test_set = vectorize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 67349 \n",
      "dev set size: 872 \n",
      "test set size: 1821\n",
      "vocab size:  12179\n"
     ]
    }
   ],
   "source": [
    "print('train set size: {} \\ndev set size: {} \\ntest set size: {}'.format(len(train_data),len(dev_data),len(test_data)))\n",
    "print('vocab size: ', len(wtoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "       # super(LoadData.self).__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx]\n",
    "        Y = int(self.labels[idx])\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    batch = list(zip(*batch))\n",
    "    \n",
    "   # lengths = torch.LongTensor([len(t) for t in batch[0]]).to(device)\n",
    "    inputs = [torch.LongTensor(t).to(device) for t in batch[0]]\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True) \n",
    "    labels = torch.LongTensor(batch[1]).to(device)\n",
    "    mask = (inputs != 0).to(device)\n",
    "    \n",
    "    return inputs, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = LoadData(train_set, train_label)\n",
    "devs = LoadData(dev_set, dev_label)\n",
    "tests = LoadData(test_set, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 64], 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=trains,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn)\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=devs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=tests,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5562,    0,    0,  ...,    0,    0,    0],\n",
       "         [1791,  470,   69,  ...,    0,    0,    0],\n",
       "         [   6,  584,   20,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [6292,   52,    6,  ...,    0,    0,    0],\n",
       "         [  75,   55, 2819,  ...,    0,    0,    0],\n",
       "         [  79,   55, 2282,  ...,    0,    0,    0]]),\n",
       " tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]),\n",
       " tensor([[ True, False, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False]]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([64, 36]),\n",
      "label shpaetorch.Size([64]),\n",
      "mask shape:torch.Size([64, 36])\n"
     ]
    }
   ],
   "source": [
    "x,y,mask = next(iter(train_loader))\n",
    "print('input shape: {},\\nlabel shpae{},\\nmask shape:{}'.format(x.shape, y.shape, mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12179 0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(wtoi)\n",
    "batch_size = 64\n",
    "emb_size = 200\n",
    "pad_idx = wtoi['PAD']\n",
    "output_size = 1\n",
    "print(vocab_size,pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, output_size, pad_idx, dropout=0.2):\n",
    "        super(SelfAttModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(emb_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1) \n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1) \n",
    "    \n",
    "    def forward(self, inputs, mask): #(bsz,seq_len)\n",
    "        # (batch_size, seq_len, emb)\n",
    "        t_emb = self.dropout(self.embed(inputs))\n",
    "        s_emb = self.dropout(self.embed(inputs))\n",
    "        mask = mask.to(float) \n",
    "        h_att = self.attention(t_emb, s_emb, mask)    \n",
    "        out = self.linear(self.dropout(h_att)).squeeze(-1) #(batch_size)\n",
    "        \n",
    "        return out#(batch_zize), \n",
    "    \n",
    "    def attention(self, emb_t, emb_s, mask=None):        \n",
    "        alpha_ts = torch.bmm(emb_s, emb_t.transpose(1,2))  #(batch_size, seq_len, emb)*(batch_size, emb, seq_len)->(bsz, s_l, s_l)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        if mask is not None:\n",
    "            alpha_ts.masked_fill_(mask == 0, -float('inf'))\n",
    "            \n",
    "        alpha_t = F.softmax(alpha_ts, dim=1) #(batch_size, seq_len, seq_len）\n",
    "            \n",
    "        h_s= torch.bmm(\n",
    "                   alpha_t,               # (batch_size, seq_len, seq_len)* (batch_size, seq_len, emb)\n",
    "                   emb_t).sum(1)               # ->(batch_size, seq_len, emb)->(batch_size, emb)\n",
    "        return h_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttModel(vocab_size, emb_size, output_size, pad_idx,dropout=0.2)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0871,  0.8914,  0.8402,  0.3348,  1.6772,  0.7354,  0.7958,  0.4505,\n",
      "        -0.3472, -0.2269, -0.7542,  1.2144,  1.0502,  0.7954,  1.2808,  0.9274,\n",
      "        -0.8962,  0.6087, -0.5044,  0.8192,  0.9652,  0.1594,  2.1019,  0.5001,\n",
      "         0.8966,  0.0722,  1.1370,  0.1226,  1.4524,  0.3237,  0.9240,  0.3178,\n",
      "         0.5811,  0.1556,  2.1860,  2.6571,  1.1220,  0.4282,  1.3611,  0.1531,\n",
      "         0.4693,  0.7239,  1.1937,  0.8048,  0.4401,  0.6421,  0.2642, -0.0731,\n",
      "        -0.3627, -1.1998,  0.1753,  0.4559,  1.1988,  0.2474,  0.9815,  0.5812,\n",
      "         0.4295,  0.6089,  1.6582,  1.0623,  0.3997,  0.9274,  0.6610, -0.1955],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "output = model(x,mask)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_cap, y):\n",
    "    preds = torch.round(torch.sigmoid(y_cap))\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.train()\n",
    "\n",
    "    for _, (inputs, labels, mask) in enumerate(data):\n",
    "        outputs = model(inputs, mask)  # (batch_size)\n",
    "        loss = criterion(outputs, labels.float()) \n",
    "        acc = binary_acc(outputs, labels)\n",
    "        \n",
    "        # sgd\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(\"batch loss: {}\".format(loss.item()))\n",
    "        \n",
    "        #epoch_loss += loss.item() * len(labels)\n",
    "        #epoch_acc += acc.item() * len(labels)\n",
    "        #total_len += len(labels)\n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc.item() \n",
    "        epoch_len = len(data)\n",
    "        \n",
    "    return epoch_loss / epoch_len, epoch_acc / epoch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.eval()\n",
    "    total_len = 0.\n",
    "    for  _, (inputs, labels, mask) in enumerate(data):\n",
    "        outputs = model(inputs, mask) \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs, mask)\n",
    "        loss = criterion(outputs, labels.float()) \n",
    "        acc = binary_acc(outputs, labels)\n",
    "        \n",
    "       # epoch_loss += loss.item() * len(labels)\n",
    "        #epoch_acc += acc.item() * len(labels)\n",
    "        #total_len += len(labels)\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc.item() \n",
    "        epoch_len = len(data)\n",
    "        \n",
    "    model.train()\n",
    "   \n",
    "    return epoch_loss / epoch_len, epoch_acc / epoch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.43349806607788444 Train Acc 0.792583830789057\n",
      "Epoch 0 Valid Loss 0.47747619450092316 Valid Acc 0.8162946445601327\n",
      "Epoch 1 Train Loss 0.2613647497787095 Train Acc 0.8958029496250896\n",
      "Epoch 1 Valid Loss 0.5788553889308657 Valid Acc 0.8234374991485051\n",
      "Epoch 2 Train Loss 0.22281245582322223 Train Acc 0.9143942477356675\n",
      "Epoch 2 Valid Loss 0.6551072171756199 Valid Acc 0.8216517865657806\n",
      "Epoch 3 Train Loss 0.20196614357848905 Train Acc 0.9224671290697422\n",
      "Epoch 3 Valid Loss 0.7302461309092385 Valid Acc 0.8029017874172756\n",
      "Epoch 4 Train Loss 0.18909954800423506 Train Acc 0.9278393931198664\n",
      "Epoch 4 Valid Loss 0.7500532673937934 Valid Acc 0.8209821454116276\n",
      "Epoch 5 Train Loss 0.18210907613704685 Train Acc 0.9306594254510921\n",
      "Epoch 5 Valid Loss 0.8082605685506549 Valid Acc 0.7991071428571429\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 6\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_loader, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"selfatt-model.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5428818048074328 0.8139863260861101\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('selfatt-model.pth'))\n",
    "test_loss, test_acc = evaluate(model,test_loader, criterion)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mistake(model, data):\n",
    "    model.eval()\n",
    "    total_len = 0.\n",
    "    for  _, (inputs, labels, mask) in enumerate(data):\n",
    "        outputs = model(inputs, mask) \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs, mask)\n",
    "            preds = torch.round(torch.sigmoid(preds))\n",
    "        \n",
    "    wrong = (preds != labels)\n",
    "    mistakes = inputs[wrong]\n",
    "    correct = labels[wrong]\n",
    "    for err, l in zip(mistakes,correct):\n",
    "        sent = [itow[w.item()] for w in err]\n",
    "        print(sent, l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['windtalker', 'blow', 'this', 'way', 'and', 'that', ',', 'but', 'there', \"'s\", 'no', 'mistake', 'the', 'filmmaker', 'in', 'the', 'tall', 'UNK', ',', 'true', 'to', 'himself', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 1\n",
      "['the', 'UNK', 'bomb', 'of', 'reggio', \"'s\", 'image', 'and', 'glass', \"'\", 'evocative', 'music', '...', 'ultimately', 'leaf', 'viewer', 'with', 'the', 'task', 'of', 'divine', 'mean', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 0\n",
      "['i', 'keep', 'think', 'over', 'and', 'over', 'again', ',', \"'\", 'i', 'should', 'be', 'enjoy', 'this', '.', \"'\", 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 0\n",
      "['the', 'pivotal', 'narrative', 'point', 'be', 'so', 'ripe', 'the', 'film', 'ca', \"n't\", 'help', 'but', 'go', 'soft', 'and', 'UNK', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 0\n",
      "['it', 'represent', 'UNK', 'movie-making', 'that', 'doe', \"n't\", 'demand', 'a', 'dumb', ',', 'distract', 'audience', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 1\n",
      "['dazzle', 'and', 'UNK', ',', 'a', 'blast', 'of', 'shallow', 'UNK', 'that', 'only', 'sex', ',', 'scandal', ',', 'and', 'a', 'UNK', 'line', 'of', 'dangerous', 'damsel', 'can', 'deliver', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'] 1\n"
     ]
    }
   ],
   "source": [
    "print_mistake(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual connection Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResAttenModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, output_size, pad_idx, dropout=0.5):\n",
    "        super(ResAttenModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(emb_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1) \n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1) \n",
    "    \n",
    "    def forward(self, inputs, mask): #(bsz*seq_len)\n",
    "        # (batch_size, seq_len, emb)\n",
    "        # (batch_size, seq_len, emb)\n",
    "        t_emb = self.dropout(self.embed(inputs))\n",
    "        s_emb = self.dropout(self.embed(inputs))\n",
    "        mask = mask.float()\n",
    "        mask = mask.unsqueeze(-1) \n",
    "        h_self = self.attention(t_emb, s_emb, mask)    \n",
    "        h_avg = self.avg(t_emb, mask)\n",
    "        h_att = h_self + h_avg\n",
    "        out = self.linear(self.dropout(h_att)).squeeze(-1) #(batch_size)\n",
    "        return out\n",
    "\n",
    "    def avg(self, x_emb, mask): #(bsz*seq_len)\n",
    "        embedded = x_emb * mask # (batch_size, seq_len, embed_size)   \n",
    "        # do avg\n",
    "        sent_emb = embedded.sum(1) / (mask.sum(1) + 1e-9)  #(batch_size, embed_size)\n",
    "        return sent_emb\n",
    "\n",
    "    def attention(self, emb_t, emb_s, mask=None):        \n",
    "        alpha_ts = torch.bmm(emb_s, emb_t.transpose(1,2))  #(batch_size, seq_len, emb)*(batch_size, emb, seq_len)->(bsz, s_l, s_l)\n",
    "        if mask is not None:\n",
    "            alpha_ts.masked_fill_(mask == 0, -float('inf'))\n",
    "            \n",
    "        alpha_t = F.softmax(alpha_ts, dim=1) #(batch_size, seq_len, seq_len）           \n",
    "        h_s= torch.bmm(\n",
    "                   alpha_t,               # (batch_size, seq_len, seq_len)* (batch_size, seq_len, emb)\n",
    "                   emb_t).sum(1)               # ->(batch_size, seq_len, emb)->(batch_size, emb)\n",
    "        return h_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResAttenModel(vocab_size, emb_size, output_size, pad_idx,dropout=0.2)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5124, -1.5075, -1.6653, -1.7425, -1.4250, -1.2171, -0.9983, -0.7353,\n",
      "        -2.4146, -0.5225,  0.0321, -1.1645, -1.2169, -1.7647, -1.2304, -0.1848,\n",
      "        -2.3924, -2.1925, -1.0917, -1.8886, -1.1039, -0.8171, -3.1461, -0.8945,\n",
      "        -2.0495, -3.1795,  0.0865, -1.5898, -1.3833, -1.1655, -2.1365, -1.3975,\n",
      "        -1.2058, -0.8670, -2.5356, -3.6437, -0.9945, -1.0295, -0.9632, -1.7956,\n",
      "        -1.8482, -1.6941, -1.1927, -0.7095, -1.1730, -0.8387, -0.8639, -1.6272,\n",
      "         0.2305, -3.1605, -0.5030, -1.7661, -1.2841, -1.3544, -1.1169, -0.8245,\n",
      "        -2.0047, -1.6963, -2.1098, -0.2487, -0.8453, -0.8668, -0.7798, -0.2529],\n",
      "       grad_fn=<SqueezeBackward1>) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "output = model(x,mask)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.416235354367258 Train Acc 0.8024973997250129\n",
      "Epoch 0 Valid Loss 0.46533111163548063 Valid Acc 0.8176339311259133\n",
      "Epoch 1 Train Loss 0.24871548343050062 Train Acc 0.9018132716049383\n",
      "Epoch 1 Valid Loss 0.5385044547063964 Valid Acc 0.8154017882687705\n",
      "Epoch 2 Train Loss 0.21176491007704115 Train Acc 0.9161324786324786\n",
      "Epoch 2 Valid Loss 0.6278615423611232 Valid Acc 0.8107142874172756\n",
      "Epoch 3 Train Loss 0.19421044020698622 Train Acc 0.9249169040835707\n",
      "Epoch 3 Valid Loss 0.6692943679434913 Valid Acc 0.8093750008514949\n",
      "Epoch 4 Train Loss 0.18202288390437082 Train Acc 0.9296207264957265\n",
      "Epoch 4 Valid Loss 0.6923431605100632 Valid Acc 0.8033482134342194\n",
      "Epoch 5 Train Loss 0.17386746837103922 Train Acc 0.933508428300095\n",
      "Epoch 5 Valid Loss 0.7529235567365374 Valid Acc 0.7910714277199337\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "N_EPOCHS = 6\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_loader, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"resatt-model.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4404615770126211 0.8230343337716728\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('resatt-model.pth'))\n",
    "test_loss, test_acc = evaluate(model,test_loader, criterion)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention model with positional encoding\n",
    "\n",
    "positional encoding 代码参考了http://nlp.seas.harvard.edu/2018/04/03/attention.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewAttModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, output_size, pad_idx, dropout=0.5):\n",
    "        super(NewAttModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.position = PositionalEncoding(emb_size)\n",
    "        self.linear = nn.Linear(emb_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1) \n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1) \n",
    "    \n",
    "    def forward(self, inputs, mask): #(bsz,seq_len)\n",
    "        # (batch_size, seq_len, emb)\n",
    "        embeded = self.embed(inputs)\n",
    "        embeded = self.position(embeded)\n",
    "        t_emb = self.dropout(embeded)\n",
    "        s_emb = self.dropout(embeded)\n",
    "        \n",
    "        mask = mask.to(float) \n",
    "        h_att = self.attention(t_emb, s_emb, mask)    \n",
    "        out = self.linear(self.dropout(h_att)).squeeze(-1) #(batch_size)\n",
    "        \n",
    "        return out#(batch_zize), \n",
    "    \n",
    "    def attention(self, emb_t, emb_s, mask=None):        \n",
    "        alpha_ts = torch.bmm(emb_s, emb_t.transpose(1,2))  #(batch_size, seq_len, emb)*(batch_size, emb, seq_len)->(bsz, s_l, s_l)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        if mask is not None:\n",
    "            alpha_ts.masked_fill_(mask == 0, -float('inf'))\n",
    "            \n",
    "        alpha_t = F.softmax(alpha_ts, dim=1) #(batch_size, seq_len, seq_len）\n",
    "            \n",
    "        h_s= torch.bmm(\n",
    "                   alpha_t,               # (batch_size, seq_len, seq_len)* (batch_size, seq_len, emb)\n",
    "                   emb_t).sum(1)               # ->(batch_size, seq_len, emb)->(batch_size, emb)\n",
    "        return h_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "        Implement the PE function.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, emb_size, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, emb_size)\n",
    "        \n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., emb_size, 2) *\n",
    "                             -(math.log(10000.0) / emb_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewAttModel(vocab_size, emb_size, output_size, pad_idx,dropout=0.5)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 1.4109000813813857 Train Acc 0.6043426095590293\n",
      "Epoch 0 Valid Loss 0.463143636073385 Valid Acc 0.7839285731315613\n",
      "Epoch 1 Train Loss 0.49624180626778636 Train Acc 0.7559622733341662\n",
      "Epoch 1 Valid Loss 0.5220595704657691 Valid Acc 0.7508928562913623\n",
      "Epoch 2 Train Loss 0.4354363615498131 Train Acc 0.7983560292695888\n",
      "Epoch 2 Valid Loss 0.44228973771844593 Valid Acc 0.8169642857142857\n",
      "Epoch 3 Train Loss 0.4015747473733491 Train Acc 0.818862207764574\n",
      "Epoch 3 Valid Loss 0.46378378782953533 Valid Acc 0.830580357994352\n",
      "Epoch 4 Train Loss 0.3760368804901074 Train Acc 0.8341770112684309\n",
      "Epoch 4 Valid Loss 0.4796773408140455 Valid Acc 0.8263392874172756\n",
      "Epoch 5 Train Loss 0.365120323465081 Train Acc 0.8421608612634744\n",
      "Epoch 5 Valid Loss 0.5081853951726641 Valid Acc 0.8133928562913623\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 6\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_loader, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"posn_atten.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4308139666401107 0.8376932226378342\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('posn_atten.pth'))\n",
    "test_loss, test_acc = evaluate(model,test_loader, criterion)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_res = []\n",
    "with open('test_results_senti.txt') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        pred = line.strip().split('\\t')[1]\n",
    "        bert_res.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Accuracy:  0.9357495881383855\n"
     ]
    }
   ],
   "source": [
    "bert_acc = sum(np.array(bert_res)==np.array(test_label))/len(bert_res)\n",
    "print('Bert Accuracy: ', bert_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
